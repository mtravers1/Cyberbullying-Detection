# -*- coding: utf-8 -*-
"""Cyberbullying.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1icFIURpANfTbxZNlGYDDAc4roM8ebgnN
"""

import pandas as pd
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth

data= pd.read_csv('/content/twitter_sexism_parsed_dataset.csv', sep='\t', error_bad_lines=False)
data1= pd.read_csv('/content/twitter_sexism_parsed_dataset.csv', sep='\t', error_bad_lines=False)
data.head()
df=pd.read_csv('/content/twitter_sexism_parsed_dataset.csv')
df1=pd.read_csv('/content/twitter_sexism_parsed_dataset.csv')

df.columns=['oh_label', 'Text']
#df.head()



"""import stop words"""

import nltk #natural language
import re
import string
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
stop_words = set(stopwords.words("english"))

st=string.punctuation #variable for puncutation
stop_words
ps=nltk.PorterStemmer()
pd.set_option("display.max_colwidth",100)

"""Clean Text and tokenize"""

from nltk.corpus import words
from nltk.corpus import stopwords
from pathlib import Path
import nltk
stopwords= nltk.corpus.stopwords.words('english')
nltk.download('stopwords')

def clean_text(text):
  #remove punctuation and convert all strings to lower
  text ="".join([word.lower() for word in str(text) if word not in string.punctuation ])
  #tokenize with Regex
  tokens=re.split('\W+', text)

  #remove stopwords and covert all words into their stems

  text=[word for word in tokens if word not in stopwords]

  return text

df['clean_text'] = df['Text'].apply(lambda x: clean_text(x))

df

"""Apply Count Vectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

count_vect=CountVectorizer(analyzer=clean_text)
X_counts=count_vect.fit_transform(df['clean_text'])
print(X_counts.shape)
print(count_vect.get_feature_names())

#X_count_df=pd.DataFrame(X_counts.toarray())
#X_count_df
#get features
#X_count_df.columns=count_vect.get_feature_names()
#X_count_df

df_sample= df[0:20]
count_vect_sample=CountVectorizer(analyzer=clean_text)
X_counts_sample=count_vect_sample.fit_transform(df_sample['Text'])
print(X_counts_sample.shape)
print(count_vect.get_feature_names())

X_counts_sample

df= pd.read_csv('/content/twitter_sexism_parsed_dataset.csv')
df

#df.columns=['oh_label', 'Text']
def clean_text(text):
  text ="".join([word.lower() for word in str(text) if word not in string.punctuation ])
  tokens=re.split('\W+', text)
  text=[word for word in tokens if word not in stopwords]
  return text

df['clean_text'] = df['Text'].apply(lambda x: clean_text(x))



"""Apply TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect=TfidfVectorizer(analyzer=clean_text)
x_tfidf=tfidf_vect.fit_transform(df['clean_text'])
print(x_tfidf.shape)
print(tfidf_vect.get_feature_names())

#df.columns=['oh_label', 'Text']
def clean_text(text):
  text ="".join([word.lower() for word in str(text) if word not in string.punctuation ])
  tokens=re.split('\W+', text)
  text=[word for word in tokens if word not in stopwords]
  return text

df['clean_text'] = df['Text'].apply(lambda x: clean_text(x))

x_tfidf_df=pd.DataFrame(x_tfidf.toarray())
x_tfidf_df.columns=tfidf_vect.get_feature_names()
#x_tfidf_df

df1_sample=df1[0:20]
tfidf_vect_sample=TfidfVectorizer(analyzer=clean_text)
x_tfidf_sample=tfidf_vect_sample.fit_transform(df1_sample['Text'])
print(x_tfidf_sample.shape)
#print(tfidf_vect.sample.get_feature_names())

df['body_len']=df['clean_text'].apply(lambda x:len(x)-x.count(''))
df.head()

df= pd.read_csv('/content/twitter_sexism_parsed_dataset.csv')
data=pd.read_csv('/content/twitter_sexism_parsed_dataset.csv')

#import string

#def count_punct(text):
   #count = sum([1 for char in text if char in string.punctuation])
   #return round(count/(len(text) - text.count(" ")), 3) * 100
#df['punct%']=df['Text'].apply(lambda x: count_punct(x))

#df.head()
import string

def count_punct(text):
  count=sum([1 for char in text if char in string.punctuation])
  return round(count/(len(text)-text.count('  ')), 3)*100     #percentage of text messagees that are punctation

df['punct%']= df['Text'].apply(lambda x: count_punct(x))

df.head()

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot
import numpy as np
# %matplotlib inline

bins = np.linspace(0, 200, 40)#creates a graph of the length of sexism and none if there was a hypothesis  that racism messages are shorter than non racism
#looking for the rows that have racism in the body_len column

pyplot.hist(df[df['oh_label']=='1']['body_len'], bins, alpha=0.5, label='sexism')
pyplot.hist(df[df['oh_label']=='0']['body_len'], bins, alpha=0.5, label='none')
pyplot.legend(loc=' upper left')
pyplot.show()

bins = np.linspace(0, 50, 40)#graph of the percentage of puntuation in racism and percentage of punctuation for non

pyplot.hist(df[df['oh_label']=='1']['punct%'], bins, alpha=0.5, label='racist')
pyplot.hist(df[df['oh_label']=='0']['punct%'], bins, alpha=0.5, label='none')
pyplot.legend(loc=' upper left')
pyplot.show()

bins = np.linspace(0,200,40)#graph of the body_len through the data

pyplot.hist(df['body_len'], bins)
pyplot.title('Body Length Distribution')
pyplot.show()

bins = np.linspace(0,50,40)#graph of punctuation percentage through the data

pyplot.hist(df['punct%'], bins)
pyplot.title('Distribution %')
pyplot.show()

for i in [1,2,3,4,5]:
  pyplot.hist((df['punct%'])**(1/i), bins=40)
  pyplot.title("Transformation: 1/{}".format(str(i)))
  pyplot.show()

#def clean_text(text):
  #text = [[words for words in sentences.lower().split()] for sentences in data]
  #text="".join([word.lower() for word in text if word not in string.punctuation])

  #tokens=re.split('\W+', text)
  #text = [words for words in sentences.lower().split()] for sentences in data]
  #text=[word for word in tokens if word not in stopwords]
  #return text

tfidf_vect=TfidfVectorizer(analyzer=clean_text)
x_tfidf = tfidf_vect.fit_transform(df["clean_text"])

count_vect=CountVectorizer(analyzer=clean_text)
X_counts=count_vect.fit_transform(df['clean_text'])

X_features = pd.concat([df['body_len'], pd.DataFrame(x_tfidf.toarray())], axis=1)#length,tfidf


X_featureslp = pd.concat([df['body_len'], df['punct%'], pd.DataFrame(x_tfidf.toarray())], axis=1)

X_features1 = pd.concat([df['body_len'], pd.DataFrame(X_counts.toarray())], axis=1)#length,tfidf

X_features1lp = pd.concat([df['body_length'], df['punct%'], pd.DataFrame(X_counts.toarray())], axis=1)#length,tfidf

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.model_selection import train_test_split

print(dir(RandomForestClassifier))
print(RandomForestClassifier)

"""Length+TFIDF"""

X_train, X_test, y_train, y_test=train_test_split(X_features,df['oh_label'], test_size=0.2)

rf2= RandomForestClassifier(n_estimators=50, max_depth =20, n_jobs=-1)
rf2_model= rf2.fit(X_train, y_train)

sorted(zip(rf2_model.feature_importances_, X_train.columns), reverse=True) [0:10]

y_pred = rf2_model.predict(X_test)
precision, recall, fscore, support= score(y_test, y_pred, pos_label='sexism', average='binary')

"""Length,Punctuaction+TFIDF"""

X_train, X_test, y_train, y_test=train_test_split(X_featureslp,df['label'], test_size=0.2)

rf2= RandomForestClassifier(n_estimators=50, max_depth =20, n_jobs=-1)
rf2_model= rf2.fit(X_train, y_train)

sorted(zip(rf2_model.feature_importances_, X_train.columns), reverse=True) [0:10]

y_pred = rf2_model.predict(X_test)
precision, recall, fscore, support= score(y_test, y_pred, pos_label='sexism', average='binary')

"""Length+count"""

X_train, X_test, y_train, y_test=train_test_split(X_features1,df['label'], test_size=0.2)

rf2= RandomForestClassifier(n_estimators=50, max_depth =20, n_jobs=-1)
rf2_model= rf2.fit(X_train, y_train)

sorted(zip(rf2_model.feature_importances_, X_train.columns), reverse=True) [0:10]

y_pred = rf2_model.predict(X_test)
precision, recall, fscore, support= score(y_test, y_pred, pos_label='sexism', average='binary')

"""length Punctuation+count"""

X_train, X_test, y_train, y_test=train_test_split(X_features1lp,df['label'], test_size=0.2)

rf2= RandomForestClassifier(n_estimators=50, max_depth =20, n_jobs=-1)
rf2_model= rf2.fit(X_train, y_train)

sorted(zip(rf2_model.feature_importances_, X_train.columns), reverse=True) [0:10]

y_pred = rf2_model.predict(X_test)
precision, recall, fscore, support= score(y_test, y_pred, pos_label='sexism', average='binary')